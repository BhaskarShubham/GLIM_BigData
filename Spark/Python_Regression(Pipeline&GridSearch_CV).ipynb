{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Regression Model\n",
    "\n",
    "In this exercise, you will implement a regression model that will predict the final score of the students.\n",
    "\n",
    "### Import Spark SQL and Spark ML Libraries\n",
    "\n",
    "First, import the libraries you will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1485153670758_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-spkcw1.vqqylijsgq3epbubbwlcp1au3h.bx.internal.cloudapp.net:8088/proxy/application_1485153670758_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.4:30060/node/containerlogs/container_1485153670758_0005_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"yarn\").appName(\"GLIM_PGPBABI\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer, MinMaxScaler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Source Data\n",
    "We will load the studentmaster Hive table into a Saprk Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "student_df = spark.sql(\"SELECT * FROM studentmaster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- studentid: integer (nullable = true)\n",
      " |-- school: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- addresstype: integer (nullable = true)\n",
      " |-- famsize: integer (nullable = true)\n",
      " |-- pstatus: integer (nullable = true)\n",
      " |-- medu: integer (nullable = true)\n",
      " |-- fedu: integer (nullable = true)\n",
      " |-- mjob: integer (nullable = true)\n",
      " |-- fjob: integer (nullable = true)\n",
      " |-- reason: integer (nullable = true)\n",
      " |-- guardian: integer (nullable = true)\n",
      " |-- traveltime: integer (nullable = true)\n",
      " |-- studytime: integer (nullable = true)\n",
      " |-- failures: integer (nullable = true)\n",
      " |-- schoolsup: integer (nullable = true)\n",
      " |-- famsup: integer (nullable = true)\n",
      " |-- paid: integer (nullable = true)\n",
      " |-- activities: integer (nullable = true)\n",
      " |-- nursery: integer (nullable = true)\n",
      " |-- higher: integer (nullable = true)\n",
      " |-- internet: integer (nullable = true)\n",
      " |-- romantic: integer (nullable = true)\n",
      " |-- famrel: integer (nullable = true)\n",
      " |-- freetime: integer (nullable = true)\n",
      " |-- goout: integer (nullable = true)\n",
      " |-- dalc: integer (nullable = true)\n",
      " |-- walc: integer (nullable = true)\n",
      " |-- health: integer (nullable = true)\n",
      " |-- absences: integer (nullable = true)\n",
      " |-- g1: integer (nullable = true)\n",
      " |-- g2: integer (nullable = true)\n",
      " |-- g3: integer (nullable = true)\n",
      " |-- smoteclass: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "student_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "Most modeling begins with exhaustive exploration and preparation of the data. In this example, the data has been cleaned for you. You will select all the columns to use as *features* and create a Boolean *label* field named **BelowMedian** with the value **1** for students scoring less than the median, or **0** if they score greater than or equal to the median score.\n",
    "\n",
    "(Note that in a real scenario, you would perform additional tasks such as handling missing or duplicated data, scaling numeric columns, and using a process called *feature engineering* to create new features for your model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = student_df.withColumnRenamed(\"g3\",\"label\").drop(\"smoteclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2320"
     ]
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- studentid: integer (nullable = true)\n",
      " |-- school: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- addresstype: integer (nullable = true)\n",
      " |-- famsize: integer (nullable = true)\n",
      " |-- pstatus: integer (nullable = true)\n",
      " |-- medu: integer (nullable = true)\n",
      " |-- fedu: integer (nullable = true)\n",
      " |-- mjob: integer (nullable = true)\n",
      " |-- fjob: integer (nullable = true)\n",
      " |-- reason: integer (nullable = true)\n",
      " |-- guardian: integer (nullable = true)\n",
      " |-- traveltime: integer (nullable = true)\n",
      " |-- studytime: integer (nullable = true)\n",
      " |-- failures: integer (nullable = true)\n",
      " |-- schoolsup: integer (nullable = true)\n",
      " |-- famsup: integer (nullable = true)\n",
      " |-- paid: integer (nullable = true)\n",
      " |-- activities: integer (nullable = true)\n",
      " |-- nursery: integer (nullable = true)\n",
      " |-- higher: integer (nullable = true)\n",
      " |-- internet: integer (nullable = true)\n",
      " |-- romantic: integer (nullable = true)\n",
      " |-- famrel: integer (nullable = true)\n",
      " |-- freetime: integer (nullable = true)\n",
      " |-- goout: integer (nullable = true)\n",
      " |-- dalc: integer (nullable = true)\n",
      " |-- walc: integer (nullable = true)\n",
      " |-- health: integer (nullable = true)\n",
      " |-- absences: integer (nullable = true)\n",
      " |-- g1: integer (nullable = true)\n",
      " |-- g2: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data\n",
    "It is common practice when building supervised machine learning models to split the source data, using some of it to train the model and reserving some to test the trained model. In this exercise, you will use 80% of the data for training, and reserve 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Rows: 1887  Testing Rows: 433"
     ]
    }
   ],
   "source": [
    "splits = data.randomSplit([0.8, 0.2])\n",
    "train = splits[0]\n",
    "test = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\n",
    "train_rows = train.count()\n",
    "test_rows = test.count()\n",
    "print \"Training Rows:\", train_rows, \" Testing Rows:\", test_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define the Pipeline\n",
    "A predictive model often requires multiple stages of feature preparation. For example, it is common when using some algorithms to distingish between continuous features (which have a calculable numeric value) and categorical features (which are numeric representations of discrete categories). It is also common to *normalize* continuous numeric features to use a common scale (for example, by scaling all numbers to a proportinal decimal value between 0 and 1).\n",
    "\n",
    "A pipeline consists of a a series of *transformer* and *estimator* stages that typically prepare a DataFrame for\n",
    "modeling and then train a predictive model. In this case, you will create a pipeline with six stages:\n",
    "- A **VectorAssembler** that combines categorical features into a single vector\n",
    "- A **VectorIndexer** that creates indexes for a vector of categorical features\n",
    "- A **VectorAssembler** that creates a vector of continuous numeric features\n",
    "- A **MinMaxScaler** that normalizes continuous numeric features\n",
    "- A **VectorAssembler** that creates a vector of categorical and continuous features\n",
    "- A **LinearRegression** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#field names to be excluded from the list of categorical variables\n",
    "remove_from_cat_input = ['studentid','age', 'absences', 'g1', 'g2', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['school', 'sex', 'addresstype', 'famsize', 'pstatus', 'medu', 'fedu', 'mjob', 'fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'dalc', 'walc', 'health']"
     ]
    }
   ],
   "source": [
    "cat_inputs = train.columns\n",
    "\n",
    "for i in remove_from_cat_input:\n",
    "    cat_inputs.remove(i)\n",
    "\n",
    "cat_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs = ['age','absences', 'g1', 'g2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "catVect = VectorAssembler(inputCols = cat_inputs, outputCol=\"catFeatures\")\n",
    "catIdx = VectorIndexer(inputCol = catVect.getOutputCol(), outputCol = \"idxCatFeatures\")\n",
    "numVect = VectorAssembler(inputCols = num_inputs, outputCol=\"numFeatures\")\n",
    "minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normFeatures\")\n",
    "featVect = VectorAssembler(inputCols=[\"idxCatFeatures\", \"normFeatures\"], outputCol=\"features\")\n",
    "lr = LinearRegression(labelCol=\"label\",featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[catVect, catIdx, numVect, minMax, featVect, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Parameters\n",
    "You can tune parameters to find the best model for your data. To do this you can use the  **CrossValidator** class to evaluate each combination of parameters defined in a **ParameterGrid** against multiple *folds* of the data split into training and validation datasets, in order to find the best performing parameters. Note that this can take a long time to run because every parameter combination is tried multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.3, 0.01]).addGrid(lr.maxIter, [10, 5]).build()\n",
    "cv = CrossValidator(estimator=pipeline, evaluator=RegressionEvaluator(), estimatorParamMaps=paramGrid, numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = cv.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "Now you're ready to apply the model to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+---------+\n",
      "|            features|        prediction|trueLabel|\n",
      "+--------------------+------------------+---------+\n",
      "|[0.0,0.0,1.0,0.0,...| 9.628018866283005|       10|\n",
      "|[0.0,0.0,1.0,0.0,...|10.452940348838975|       10|\n",
      "|(32,[2,4,5,6,8,10...|12.714638998809821|       13|\n",
      "|(32,[2,5,6,7,8,9,...| 11.88654685222408|       11|\n",
      "|(32,[2,4,5,6,7,8,...|14.139191553579556|       14|\n",
      "|(32,[2,5,6,7,8,9,...| 9.334267264535855|        9|\n",
      "|(32,[2,4,5,6,7,8,...|11.441449616039977|       11|\n",
      "|[0.0,0.0,1.0,0.0,...|  16.0856559485407|       16|\n",
      "|(32,[2,4,5,6,7,8,...|  15.6423285283139|       15|\n",
      "|[0.0,0.0,1.0,0.0,...|10.585249122132774|       12|\n",
      "|[0.0,0.0,1.0,0.0,...|13.836816150581921|       13|\n",
      "|[0.0,0.0,1.0,0.0,...|12.215546702899967|       15|\n",
      "|(32,[2,4,5,6,7,8,...|12.044995881019481|       14|\n",
      "|[0.0,0.0,1.0,0.0,...|14.917163456141093|       17|\n",
      "|[0.0,0.0,1.0,0.0,...|12.429675508592256|       13|\n",
      "|[0.0,0.0,1.0,0.0,...| 16.81304403947036|       17|\n",
      "|[0.0,0.0,1.0,0.0,...|17.394466378805774|       17|\n",
      "|(32,[4,5,6,7,8,9,...|11.042335405947195|       11|\n",
      "|[0.0,0.0,0.0,0.0,...|12.641408379107821|       12|\n",
      "|[0.0,0.0,0.0,0.0,...|  7.53413211378589|       10|\n",
      "+--------------------+------------------+---------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(test)\n",
    "predicted = prediction.select(\"features\", \"prediction\", \"trueLabel\")\n",
    "predicted.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the Predicted and Actual Values\n",
    "You can plot the predicted values against the actual values to see how accurately the model has predicted. In a perfect model, the resulting scatter plot should form a perfect diagonal line with each predicted value being identical to the actual value - in practice, some variance is to be expected.\n",
    "Run the cells below to create a temporary table from the **predicted** DataFrame and then retrieve the predicted and actual label values using SQL. You can then display the results as a scatter plot, specifying **-** as the function to show the unaggregated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted.createOrReplaceTempView(\"regressionPredictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT trueLabel, prediction FROM regressionPredictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Root Mean Square Error (RMSE)\n",
    "There are a number of metrics used to measure the variance between predicted and actual values. Of these, the root mean square error (RMSE) is a commonly used value that is measured in the same units as the prediced and actual values - so in this case, the RMSE indicates the average difference between predicted and actual scores obtained in the final exams. You can use the **RegressionEvaluator** class to retrieve the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error (RMSE): 1.1372543448"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print \"Root Mean Square Error (RMSE):\", rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  },
  "widgets": {
   "state": {
    "b7b45d05ef084da4afbb4d560f917f15": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "df26534e7ae14d3db2583a29c2173ff3": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}